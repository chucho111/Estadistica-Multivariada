{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mahotas\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a fix file size\n",
    "fixed_size  = tuple((512,512))\n",
    "\n",
    "#train path \n",
    "train_path = \"train/rgb\"\n",
    "test_path = \"test/rgb\"\n",
    "\n",
    "# no of trees for Random Forests\n",
    "num_tree = 25\n",
    "\n",
    "# bins for histograms \n",
    "bins = 8\n",
    "\n",
    "# train_test_split size\n",
    "test_size = 0.20\n",
    "\n",
    "# seed for reproducing same result \n",
    "seed = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features description -1:  Hu Moments\n",
    "\n",
    "def fd_hu_moments(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor -2 Haralick Texture \n",
    "\n",
    "def fd_haralick(image):\n",
    "    # conver the image to grayscale\n",
    "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    # Ccompute the haralick texture fetature ve tor \n",
    "    haralic = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    return haralic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-description -3 Color Histogram\n",
    "\n",
    "def fd_histogram(image, mask=None):\n",
    "    # conver the image to HSV colors-space\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    #COPUTE THE COLOR HISTPGRAM\n",
    "    hist  = cv2.calcHist([image],[0,1,2],None,[bins,bins,bins], [0, 256, 0, 256, 0, 256])\n",
    "    # normalize the histogram\n",
    "    cv2.normalize(hist,hist)\n",
    "    # return the histog....\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['double_plant', 'drydown', 'endrow', 'nutrient_deficiency', 'planter_skip', 'storm_damage', 'water', 'waterway', 'weed_cluster']\n"
     ]
    }
   ],
   "source": [
    "# get the training data labels \n",
    "train_labels = os.listdir(train_path)\n",
    "test_labels = os.listdir(test_path)\n",
    "\n",
    "# sort the training labesl \n",
    "train_labels.sort()\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to hold feature vectors and labels \n",
    "global_features = []\n",
    "labels = []\n",
    "i, j = 0, 0 \n",
    "k = 0\n",
    "\n",
    "# num of images per class \n",
    "images_per_class = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loop insise the folder for train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "[STATUS] processed folder: double_plant\n",
      "[STATUS] processed folder: drydown\n",
      "[STATUS] processed folder: endrow\n",
      "[STATUS] processed folder: nutrient_deficiency\n",
      "[STATUS] processed folder: planter_skip\n",
      "[STATUS] processed folder: storm_damage\n",
      "[STATUS] processed folder: water\n",
      "[STATUS] processed folder: waterway\n",
      "[STATUS] processed folder: weed_cluster\n",
      "[STATUS] completed Global Feature Extraction...\n"
     ]
    }
   ],
   "source": [
    "# ittirate the folder to get the image label name\n",
    "\n",
    "%time\n",
    "# lop over the training data sub folder \n",
    "\n",
    "for training_name in train_labels:\n",
    "    # join the training data path and each species training folder\n",
    "    dir = os.path.join(train_path, training_name)\n",
    "    \n",
    "    # get the current training label\n",
    "    current_label = training_name\n",
    "    \n",
    "    k = 1\n",
    "    # loop over the images in each sub-folder\n",
    "    \n",
    "    for file in os.listdir(dir):\n",
    "        file = dir + \"/\" + os.fsdecode(file)\n",
    "        \n",
    "        # read the image and resize it to a fixed-size\n",
    "        image = cv2.imread(file)\n",
    "        nir = cv2.imread(file.replace('rgb','nir'))\n",
    "        \n",
    "        if image is not None:\n",
    "            image = cv2.resize(image,fixed_size)\n",
    "            fv_hu_moments = fd_hu_moments(image)\n",
    "            fv_haralick   = fd_haralick(image)\n",
    "            fv_histogram  = fd_histogram(image)\n",
    "            nir = cv2.resize(nir,fixed_size)\n",
    "            fv_hu_moments2 = fd_hu_moments(nir)\n",
    "            fv_haralick2   = fd_haralick(nir)\n",
    "            fv_histogram2  = fd_histogram(nir)\n",
    "            \n",
    "        # Concatenate global features\n",
    "        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments, fv_histogram2, fv_haralick2, fv_hu_moments2])\n",
    "        \n",
    "        # update the list of labels and feature vectors\n",
    "        labels.append(current_label)\n",
    "        global_features.append(global_feature)\n",
    "        \n",
    "        i += 1\n",
    "        k += 1\n",
    "    print(\"[STATUS] processed folder: {}\".format(current_label))\n",
    "    j += 1\n",
    "\n",
    "print(\"[STATUS] completed Global Feature Extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "plt.savefigure('muestra1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "[STATUS] feature vector size (1080, 1064)\n",
      "[STATUS] training Labels (1080,)\n",
      "[STATUS] training labels encoded...{}\n",
      "[STATUS] feature vector normalized...\n",
      "[STATUS] target labels: [0 0 0 ... 8 8 8]\n",
      "[STATUS] target labels shape: (1080,)\n",
      "[STATUS] end of training..\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# get the overall feature vector size\n",
    "print(\"[STATUS] feature vector size {}\".format(np.array(global_features).shape))\n",
    "\n",
    "# get the overall training label size\n",
    "print(\"[STATUS] training Labels {}\".format(np.array(labels).shape))\n",
    "\n",
    "# encode the target labels\n",
    "targetNames = np.unique(labels)\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(labels)\n",
    "print(\"[STATUS] training labels encoded...{}\")\n",
    "\n",
    "# normalize the feature vector in the range (0-1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaled_features = scaler.fit_transform(global_features)\n",
    "print(\"[STATUS] feature vector normalized...\")\n",
    "\n",
    "print(\"[STATUS] target labels: {}\".format(target))\n",
    "print(\"[STATUS] target labels shape: {}\".format(target.shape))\n",
    "\n",
    "# save the feature vector using HDF5\n",
    "h5f_data = h5py.File('data.h5', 'w')\n",
    "h5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))\n",
    "\n",
    "h5f_label = h5py.File('labels.h5', 'w')\n",
    "h5f_label.create_dataset('dataset_1', data=np.array(target))\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "print(\"[STATUS] end of training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the feature vector and trained labels\n",
    "\n",
    "h5f_data = h5py.File('data.h5', 'r')\n",
    "h5f_label = h5py.File('labels.h5', 'r')\n",
    "\n",
    "global_features_string = h5f_data['dataset_1']\n",
    "global_labels_string = h5f_label['dataset_1']\n",
    "\n",
    "global_features = np.array(global_features_string)\n",
    "global_labels = np.array(global_labels_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training and testing data\n",
    "(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),\n",
    "                                                                                          np.array(global_labels),\n",
    "                                                                                          test_size=test_size,\n",
    "                                                                                          random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        95\n",
      "           1       1.00      1.00      1.00       101\n",
      "           2       1.00      1.00      1.00        94\n",
      "           3       1.00      1.00      1.00        95\n",
      "           4       1.00      1.00      1.00        96\n",
      "           5       1.00      1.00      1.00       104\n",
      "           6       1.00      1.00      1.00        96\n",
      "           7       1.00      1.00      1.00        87\n",
      "           8       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           1.00       864\n",
      "   macro avg       1.00      1.00      1.00       864\n",
      "weighted avg       1.00      1.00      1.00       864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the model - Random Forests\n",
    "clf  = RandomForestClassifier(n_estimators=num_tree, max_features=None, max_depth=None)\n",
    "\n",
    "# fit the training data to the model\n",
    "clf.fit(trainDataGlobal, trainLabelsGlobal)\n",
    "\n",
    "#print(clf.fit(trainDataGlobal, trainLabelsGlobal))\n",
    "clf_pred = clf.predict(trainDataGlobal)\n",
    "\n",
    "print(classification_report(trainLabelsGlobal,clf_pred))\n",
    "#print(confusion_matrix(trainLabelsGlobal,clf_pred))\n",
    "\n",
    "#print(clf.predict(trainDataGlobal))\n",
    "\n",
    "#print(clf.predict(global_feature.reshape(1,-1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 95   0   0   0   0   0   0   0   0]\n",
      " [  0 101   0   0   0   0   0   0   0]\n",
      " [  0   0  94   0   0   0   0   0   0]\n",
      " [  0   0   0  95   0   0   0   0   0]\n",
      " [  0   0   0   0  96   0   0   0   0]\n",
      " [  0   0   0   0   0 104   0   0   0]\n",
      " [  0   0   0   0   0   0  96   0   0]\n",
      " [  0   0   0   0   0   0   0  87   0]\n",
      " [  0   0   0   0   0   0   0   0  96]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(trainLabelsGlobal,clf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.10      0.14        50\n",
      "           1       1.00      0.78      0.88        50\n",
      "           2       0.00      0.00      0.00        50\n",
      "           3       0.29      0.68      0.40        50\n",
      "           4       0.27      0.84      0.41        50\n",
      "           5       0.17      0.10      0.12        50\n",
      "           6       0.96      0.50      0.66        50\n",
      "           7       0.56      0.36      0.44        50\n",
      "           8       0.67      0.32      0.43        50\n",
      "\n",
      "    accuracy                           0.41       450\n",
      "   macro avg       0.46      0.41      0.39       450\n",
      "weighted avg       0.46      0.41      0.39       450\n",
      "\n",
      "[[ 5  0  0  0 43  0  0  0  2]\n",
      " [ 0 39  0  0 10  0  0  0  1]\n",
      " [ 4  0  0 13 24  4  0  3  2]\n",
      " [ 0  0  2 34  0  6  0  8  0]\n",
      " [ 3  0  0  2 42  0  1  0  2]\n",
      " [ 1  0  0 43  0  5  0  1  0]\n",
      " [ 4  0  1 11  5  1 25  2  1]\n",
      " [ 1  0  1 16  0 14  0 18  0]\n",
      " [ 4  0  0  0 30  0  0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# path to test data\n",
    "predictions = []\n",
    "test_label = []\n",
    "i = 0\n",
    "# loop through the test images\n",
    "#for file in glob.glob(test_path + \"/*.jpg\"):\n",
    "for test_name in test_labels:\n",
    "    dir = os.path.join(test_path, test_name)\n",
    "    \n",
    "    #get the current test Label\n",
    "    current_label = test_name\n",
    "    # loop through the test images\n",
    "    for file in os.listdir(dir):\n",
    "        file = dir + '/' + os.fsdecode(file)\n",
    "        \n",
    "        #read the image\n",
    "        image = cv2.imread(file)\n",
    "        #resize the image\n",
    "        image = cv2.resize(image, fixed_size)\n",
    "        \n",
    "        nir = cv2.imread(file.replace('rgb','nir'))\n",
    "        nir = cv2.resize(nir, fixed_size)\n",
    "        \n",
    "        #Global Feature extraction\n",
    "        fv_hu_moments = fd_hu_moments(image)\n",
    "        fv_haralick   = fd_haralick(image)\n",
    "        fv_histogram  = fd_histogram(image)\n",
    "        fv_hu_moments2 = fd_hu_moments(nir)\n",
    "        fv_haralick2   = fd_haralick(nir)\n",
    "        fv_histogram2  = fd_histogram(nir)\n",
    "        \n",
    "        #Concatenate global features\n",
    "        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments, fv_histogram2, fv_haralick2, fv_hu_moments2])\n",
    "        \n",
    "        # predict label of test image\n",
    "        prediction = clf.predict(global_feature.reshape(1,-1))[0]\n",
    "        predictions.append(prediction)\n",
    "        #print(prediction)\n",
    "        test_label.append(i)\n",
    "\n",
    "        # show predicted label on image\n",
    "        cv2.putText(image, train_labels[prediction], (20,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,255), 3)\n",
    "        cv2.putText(image, train_labels[i], (20,60), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 3)\n",
    "        # display the output image\n",
    "        #plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        #plt.show()\n",
    "    i += 1\n",
    "print(classification_report(test_label,predictions)) \n",
    "print(confusion_matrix(test_label,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
